# Local AI Models - Context Window Analysis for VISTA

## üìä VISTA's Actual Request Sizes

Based on code analysis, here's what VISTA sends to AI:

### Typical Request Components

1. **System Prompt**: ~500-1,000 tokens
   - Role definition
   - Task instructions
   - Output format requirements

2. **Request Analysis** (from DeepRequestAnalyzer):
   - HTTP method, URL, path: ~50-100 tokens
   - Headers (10-20 headers): ~200-400 tokens
   - Parameters (URL + Body + Cookies): ~300-1,000 tokens
   - Authentication info: ~100-200 tokens
   - Technology detection: ~100-200 tokens
   - Risk scoring: ~100-200 tokens
   - **Subtotal**: ~850-2,100 tokens

3. **Response Analysis** (from ResponseAnalyzer):
   - Status code, headers: ~200-400 tokens
   - Security headers analysis: ~300-500 tokens
   - Error messages: ~200-500 tokens
   - Sensitive data detection: ~200-400 tokens
   - Response body (truncated): ~500-2,000 tokens
   - **Subtotal**: ~1,400-3,800 tokens

4. **Bypass Engine Context** (when used):
   - Original payload: ~50-200 tokens
   - Blocking analysis: ~200-400 tokens
   - WAF detection: ~100-200 tokens
   - Previous attempts: ~500-1,500 tokens
   - **Subtotal**: ~850-2,300 tokens

5. **User Query**: ~100-500 tokens

### Total Request Sizes by Feature

| Feature | Minimum Tokens | Typical Tokens | Maximum Tokens |
|---------|---------------|----------------|----------------|
| **Simple Analysis** | 1,500 | 3,000 | 5,000 |
| **Deep Analysis** | 3,000 | 6,000 | 10,000 |
| **Bypass Engine** | 4,000 | 8,000 | 15,000 |
| **Multi-Request** | 6,000 | 12,000 | 25,000 |
| **Collection Analysis** | 8,000 | 15,000 | 40,000 |

### Real-World Examples

#### Example 1: Simple Request Analysis
```
System Prompt: 500 tokens
Request Data: 1,200 tokens
Response Data: 1,800 tokens
User Query: 200 tokens
---
Total: ~3,700 tokens
```

#### Example 2: Bypass Engine with Multiple Attempts
```
System Prompt: 800 tokens
Original Request: 1,500 tokens
Blocking Analysis: 600 tokens
10 Bypass Attempts: 3,000 tokens
Response Analysis: 2,000 tokens
User Query: 300 tokens
---
Total: ~8,200 tokens
```

#### Example 3: Collection Analysis (5 requests)
```
System Prompt: 600 tokens
5 Requests √ó 1,500 tokens: 7,500 tokens
5 Responses √ó 2,000 tokens: 10,000 tokens
Pattern Analysis: 1,500 tokens
User Query: 400 tokens
---
Total: ~20,000 tokens
```

---

## üéØ Updated Model Recommendations

### Context Window Requirements

Based on VISTA's actual usage:

| Use Case | Min Context | Recommended Context | Ideal Context |
|----------|-------------|---------------------|---------------|
| Simple Analysis | 8K | 16K | 32K |
| Deep Analysis | 16K | 32K | 64K |
| Bypass Engine | 32K | 64K | 128K |
| Multi-Request | 64K | 128K | 200K |
| Collection Analysis | 128K | 200K | 1M |

---

## üèÜ REVISED Model Rankings

### Tier 1: Best for VISTA (128K+ Context)

#### 1. **Llama 3.1 8B** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê BEST OVERALL
- **Context Window**: 128K tokens
- **Size**: ~5GB
- **RAM**: 8GB minimum, 16GB recommended
- **Speed**: Fast (20-40 tokens/sec on 16GB RAM)
- **Quality**: Excellent reasoning

**Why Best for VISTA**:
- ‚úÖ 128K context handles ALL VISTA use cases
- ‚úÖ Excellent at security analysis
- ‚úÖ Fast enough for interactive use
- ‚úÖ Good at code understanding
- ‚úÖ Handles multi-request analysis perfectly

**Use Cases**:
- ‚úÖ Simple analysis (3K tokens) - Plenty of room
- ‚úÖ Deep analysis (10K tokens) - Comfortable
- ‚úÖ Bypass engine (15K tokens) - No problem
- ‚úÖ Multi-request (25K tokens) - Easy
- ‚úÖ Collection analysis (40K tokens) - Still has room

**Ollama**: `ollama pull llama3.1:8b`

---

#### 2. **Qwen 2.5 32B** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê BEST QUALITY
- **Context Window**: 128K tokens
- **Size**: ~19GB
- **RAM**: 32GB minimum
- **Speed**: Medium (10-20 tokens/sec on 32GB RAM)
- **Quality**: Outstanding

**Why Excellent for VISTA**:
- ‚úÖ 128K context handles everything
- ‚úÖ Best code understanding
- ‚úÖ Excellent at payload generation
- ‚úÖ Superior reasoning

**Trade-off**: Requires more RAM but better quality

**Ollama**: `ollama pull qwen2.5:32b`

---

### Tier 2: Good for VISTA (32K-64K Context)

#### 3. **Mistral 7B** ‚≠ê‚≠ê‚≠ê‚≠ê
- **Context Window**: 32K tokens
- **Size**: ~4GB
- **RAM**: 8GB minimum
- **Speed**: Very Fast (30-50 tokens/sec)
- **Quality**: Good

**Why Good for VISTA**:
- ‚úÖ Fast responses
- ‚úÖ Handles simple/deep analysis well
- ‚ö†Ô∏è Struggles with large collections (40K tokens)
- ‚ö†Ô∏è May need truncation for multi-request

**Best For**: Quick analysis, single requests

**Ollama**: `ollama pull mistral:7b`

---

#### 4. **DeepSeek-R1 14B** ‚≠ê‚≠ê‚≠ê‚≠ê
- **Context Window**: 64K tokens
- **Size**: ~8GB
- **RAM**: 16GB minimum
- **Speed**: Medium (15-25 tokens/sec)
- **Quality**: Excellent reasoning

**Why Good for VISTA**:
- ‚úÖ Good context window (64K)
- ‚úÖ Excellent at complex analysis
- ‚úÖ Handles bypass engine well
- ‚ö†Ô∏è May struggle with large collections

**Best For**: Complex vulnerability analysis, exploit chains

**Ollama**: `ollama pull deepseek-r1:14b`

---

### Tier 3: Limited for VISTA (8K-16K Context)

#### 5. **Gemma 2 9B** ‚≠ê‚≠ê‚≠ê
- **Context Window**: 8K tokens
- **Size**: ~5GB
- **RAM**: 10GB minimum
- **Speed**: Fast (25-40 tokens/sec)
- **Quality**: Good

**Why Limited for VISTA**:
- ‚ö†Ô∏è Only 8K context - too small for many use cases
- ‚ö†Ô∏è Will truncate deep analysis
- ‚ö†Ô∏è Cannot handle multi-request
- ‚ö†Ô∏è Cannot handle collections

**Best For**: Very simple, single-request analysis only

**Ollama**: `ollama pull gemma2:9b`

---

## üìä Detailed Comparison Table

| Model | Context | RAM | Speed | Quality | Simple | Deep | Bypass | Multi | Collection |
|-------|---------|-----|-------|---------|--------|------|--------|-------|------------|
| **Llama 3.1 8B** | 128K | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Qwen 2.5 32B** | 128K | 32GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Mistral 7B** | 32K | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ùå |
| **DeepSeek-R1 14B** | 64K | 16GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è |
| **Gemma 2 9B** | 8K | 10GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚úÖ | ‚ö†Ô∏è | ‚ùå | ‚ùå | ‚ùå |

Legend:
- ‚úÖ = Fully supported
- ‚ö†Ô∏è = Partially supported (may need truncation)
- ‚ùå = Not supported (context too small)

---

## üí° Smart Recommendations by Hardware

### 8GB RAM Users

**Primary**: **Llama 3.1 8B** ‚≠ê
- Handles all VISTA features
- 128K context is perfect
- Fast enough for interactive use

**Alternative**: Mistral 7B (if speed is priority)
- Faster but limited context (32K)
- Good for simple analysis only

### 16GB RAM Users

**Primary**: **Llama 3.1 8B** ‚≠ê
- Runs very smoothly
- Excellent performance
- Handles everything

**Alternative**: DeepSeek-R1 14B
- Better reasoning
- 64K context (still good)
- Slightly slower

### 32GB+ RAM Users

**Primary**: **Qwen 2.5 32B** ‚≠ê
- Best quality
- 128K context
- Worth the extra RAM

**Alternative**: Llama 3.1 70B
- Even better quality
- 128K context
- Requires 48GB+ RAM

---

## üéØ Final Recommendation

### Default Model for VISTA: **Llama 3.1 8B**

**Why**:
1. ‚úÖ **128K context** - Handles ALL VISTA use cases perfectly
2. ‚úÖ **8GB RAM** - Accessible to most users
3. ‚úÖ **Fast** - Good interactive performance
4. ‚úÖ **Quality** - Excellent for security testing
5. ‚úÖ **Popular** - 108M+ downloads, well-tested

### Context Window Strategy

**Implement Smart Truncation**:
```java
// Pseudo-code for context management
if (totalTokens > modelContextWindow * 0.8) {
    // Truncate oldest/least important data
    // Keep: System prompt, latest request, user query
    // Truncate: Old requests, verbose responses
}
```

**Priority Order** (what to keep when truncating):
1. System prompt (always keep)
2. Current request/response (always keep)
3. User query (always keep)
4. Recent bypass attempts (keep last 5)
5. Historical requests (truncate first)
6. Verbose response bodies (summarize)

---

## üîß Implementation Recommendations

### 1. Model Selection UI

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Local AI Settings                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Model: [llama3.1:8b          ‚ñº]         ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ Context Window: 128K tokens             ‚îÇ
‚îÇ Estimated RAM: 8GB                      ‚îÇ
‚îÇ Speed: Fast (20-40 tokens/sec)          ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ ‚úÖ Supports all VISTA features          ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ [üîÑ Refresh Models] [‚ÑπÔ∏è Model Info]    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. Feature Compatibility Check

```java
public boolean isModelCompatible(String feature, String model) {
    int requiredContext = getFeatureContextRequirement(feature);
    int modelContext = getModelContextWindow(model);
    return modelContext >= requiredContext;
}
```

### 3. Warning System

```
‚ö†Ô∏è Warning: Selected model (Gemma 2 9B) has only 8K context.
   Collection analysis requires 128K context.
   
   Recommended: Switch to Llama 3.1 8B (128K context)
   
   [Switch Model] [Continue Anyway]
```

---

## üìà Performance Benchmarks

### Llama 3.1 8B Performance

| Hardware | Tokens/Sec | 3K Request | 10K Request | 25K Request |
|----------|------------|------------|-------------|-------------|
| 8GB RAM | 15-20 | 3-4 sec | 8-10 sec | 20-25 sec |
| 16GB RAM | 25-35 | 2-3 sec | 5-7 sec | 12-15 sec |
| 32GB RAM | 35-45 | 1-2 sec | 3-5 sec | 8-10 sec |

### Comparison: Cloud vs Local

| Metric | GPT-4 (Cloud) | Llama 3.1 8B (Local) |
|--------|---------------|----------------------|
| **Latency** | 1-3 sec | 0 sec (no network) |
| **3K Request** | 2-4 sec | 2-3 sec (16GB RAM) |
| **10K Request** | 4-8 sec | 5-7 sec (16GB RAM) |
| **25K Request** | 8-15 sec | 12-15 sec (16GB RAM) |
| **Quality** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Cost** | $0.30-0.60 | Free |
| **Privacy** | ‚ö†Ô∏è Cloud | ‚úÖ Local |

---

## üéì Conclusion

**For VISTA, context window is CRITICAL.**

**Best Choice**: **Llama 3.1 8B**
- 128K context handles everything
- 8GB RAM is accessible
- Fast enough for interactive use
- Excellent quality

**Alternative for Power Users**: **Qwen 2.5 32B**
- Same 128K context
- Better quality
- Requires 32GB RAM

**Avoid**: Models with <32K context (Gemma 2, older models)
- Will truncate important data
- Poor user experience
- Limited functionality

---

**Implementation Priority**: Add context window detection and warnings! üö®
